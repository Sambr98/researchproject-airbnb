{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to build platform-specific dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import itertools\n",
    "import gensim\n",
    "\n",
    "from scripts import preprocess_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = pd.read_csv(\"../../data/airbnb_listings_description/london_listings_description_ward.csv\")[['full_description','ward']]\n",
    "descriptions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ward_count = pd.DataFrame(descriptions['ward'].value_counts()).reset_index().rename(columns={\"index\": \"ward\", \"ward\": \"count\"})\n",
    "ward_count = ward_count[ward_count['count'] < 5].reset_index(drop=True)\n",
    "ethic_wards = ward_count['ward'].tolist()\n",
    "\n",
    "rows = []\n",
    "for i in range(descriptions.shape[0]):\n",
    "    if (descriptions['ward'][i] in ethic_wards):\n",
    "        rows.append(i)\n",
    "descriptions = descriptions.drop(rows).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "punctuation = ['.',',',')','(','!',':',';']\n",
    "def hasNumbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "def verifyWord(word):\n",
    "    return (word not in stopwords) and (word not in punctuation) and (not hasNumbers(word))\n",
    "dict_filter = lambda word_freq, stopwords: dict((word,word_freq[word]) for word in word_freq if verifyWord(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the top 150 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"\"\n",
    "for i in range(descriptions.shape[0]):\n",
    "    doc += descriptions['full_description'][i]\n",
    "    doc += \" \"\n",
    "doc = doc[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = preprocess_text(doc)\n",
    "tokens = nltk.word_tokenize(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = nltk.FreqDist(tokens)\n",
    "filtered_word_freq = dict_filter(word_freq, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top150 = [item[0] for item in list(itertools.islice(filtered_word_freq.items(), 0, 150))]\n",
    "print(top150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Word2Vec on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning and tokenizing the descriptions\n",
    "descriptions_clean = descriptions.copy()\n",
    "for i in range(descriptions.shape[0]):\n",
    "    val = [nltk.word_tokenize(s) for s in nltk.sent_tokenize(preprocess_text(descriptions['full_description'][i]))]\n",
    "    descriptions_clean['full_description'][i] = val\n",
    "descriptions_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the corpus of sentences\n",
    "corpus = []\n",
    "for desc in range(descriptions_clean.shape[0]):\n",
    "    sentences = descriptions_clean['full_description'][desc]\n",
    "    for i in sentences:\n",
    "        corpus.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = gensim.models.Word2Vec(sentences=corpus, window=3, min_count=5, sg=1, iter=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "expansion = []\n",
    "for key in top150:\n",
    "    similarities = word2vec_model.wv.most_similar(key, topn=None)\n",
    "    for i in range(similarities.shape[0]):\n",
    "        if (similarities[i] > 0.75):\n",
    "            val = word2vec_model.wv.index2word[i]\n",
    "            if (val not in top150 and val not in expansion and verifyWord(val)):\n",
    "                expansion.append(val)\n",
    "print(expansion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output the dictionary to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_expanded = top150 + expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expanded = pd.DataFrame(data={\"tokens\": dict_expanded})\n",
    "df_expanded.to_csv(\"../../data/dictionary/london.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
