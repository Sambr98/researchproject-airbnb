{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to make estimations with Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywpYFY5Fye1w"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "import copy\n",
    "from scipy import stats\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext as tt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from scripts import preprocess_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZVRw_VwI54o7",
    "outputId": "55eb088e-eeed-4ee4-cb14-10c30ce56dcc"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oBD2b5Y6ykPm",
    "outputId": "b693439e-d495-459f-b9d6-d6008c1563b2"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfIJLldH29VR"
   },
   "source": [
    "## Importing the necessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lNMrEiRvyeEI"
   },
   "outputs": [],
   "source": [
    "embeddings = KeyedVectors.load_word2vec_format('../../data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mCFAutvcXyJ7"
   },
   "outputs": [],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3HfHPLos2K2P"
   },
   "outputs": [],
   "source": [
    "descriptions_ = pd.read_csv(\"../../data/airbnb_listings_description/london_listings_description_ward.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMHAzsmS2YHe"
   },
   "outputs": [],
   "source": [
    "imd_per_ward = pd.read_csv(\"../../data/imd_per_ward.csv\")[['WD17CD','Index of Multiple Deprivation (IMD) Score','Education, Skills and Training Score','Employment Score (rate)','Income Score (rate)']]\n",
    "imd_per_ward = imd_per_ward.rename(columns={\"Index of Multiple Deprivation (IMD) Score\": \"IMD\", \"Education, Skills and Training Score\" : \"IMD_Edu\", 'Employment Score (rate)' : 'IMD_Emp', 'Income Score (rate)': 'IMD_Inc'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing some wards (Ethics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_per_ward = descriptions_.groupby('ward', as_index=False).agg(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wards = []\n",
    "for i in range(descriptions_per_ward.shape[0]):\n",
    "    if (len(descriptions_per_ward['full_description'][i]) < 5):\n",
    "        wards.append(descriptions_per_ward['ward'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i in range(descriptions_.shape[0]):\n",
    "    if (descriptions_['ward'][i] in wards):\n",
    "        rows.append(i)\n",
    "descriptions = descriptions_.drop(rows).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkYaHnad1_0n"
   },
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XsXbKShh5B51"
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FPyd3lvMjzar"
   },
   "outputs": [],
   "source": [
    "# Function to get a subset of the pre-trained Word2Vec library (Not to overload CPU/GPU)\n",
    "def get_embeddings_subset(w2v, word_list):\n",
    "  for i in range(len(word_list)):\n",
    "    if (word_list[i] not in embeddings.vocab):\n",
    "      word_list[i] = 'UNK'\n",
    "  word_list = list(set(word_list))\n",
    "  \n",
    "  vectors = []\n",
    "  for token in word_list:\n",
    "    vectors.append(w2v[token])\n",
    "  \n",
    "  embeddings_sub = KeyedVectors(300)\n",
    "  zero_vec = np.zeros(300)\n",
    "  embeddings_sub.add('<0>', zero_vec)\n",
    "  embeddings_sub.add(word_list, vectors)\n",
    "\n",
    "  return embeddings_sub\n",
    "\n",
    "# Function to get the index of each word in the pre-trained library\n",
    "def get_embeddings_idx(data, w2v):\n",
    "  data_idx = []\n",
    "  for i in data:\n",
    "    current_sequence = []\n",
    "    for token in i:\n",
    "      if (token in w2v.vocab):\n",
    "        token_to_find = token\n",
    "      else:\n",
    "        token_to_find = 'UNK'\n",
    "      current_sequence.append(w2v.vocab[token_to_find].index)\n",
    "    data_idx.append(current_sequence)\n",
    "  return data_idx\n",
    "\n",
    "# Function to batchify data per description length\n",
    "def batchify_per_len(x, y, max_length):\n",
    "  lengths = []\n",
    "  for i in x:\n",
    "    lengths.append(len(i))\n",
    "  lengths = list(set(lengths))\n",
    "\n",
    "  batches_x = []\n",
    "  batches_y = []\n",
    "  for l in lengths:\n",
    "    current_x_batch = []\n",
    "    current_y_batch = []\n",
    "    for item in range(len(x)):\n",
    "      if (len(x[item]) == l):\n",
    "        current_x_batch.append(x[item])\n",
    "        current_y_batch.append(y[item])\n",
    "      if (len(current_x_batch) == max_length):\n",
    "        batches_x.append(current_x_batch)\n",
    "        batches_y.append(current_y_batch)\n",
    "        current_x_batch = []\n",
    "        current_y_batch = []\n",
    "    if (len(current_x_batch) != 0):\n",
    "      batches_x.append(current_x_batch)\n",
    "      batches_y.append(current_y_batch)\n",
    "\n",
    "  return (batches_x, batches_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkpNjgpo5EEN"
   },
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "jH2HZKPJ2z94",
    "outputId": "86862471-1f9c-4a99-901c-f52eef472ce6"
   },
   "outputs": [],
   "source": [
    "descriptions_imd = descriptions.merge(imd_per_ward, left_on='ward', right_on=\"WD17CD\").drop(['id', 'ward', 'WD17CD', 'IMD_Edu', 'IMD_Emp', 'IMD_Inc'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l6qpvXzejhmb"
   },
   "outputs": [],
   "source": [
    "X_original = descriptions_imd['full_description'].tolist()\n",
    "y_original = descriptions_imd['IMD'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMSLOO-zjhpd"
   },
   "outputs": [],
   "source": [
    "X_tokens = []\n",
    "for i in X_original:\n",
    "    X_tokens.append(nltk.word_tokenize(preprocess_text(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MI1P2329jhtS"
   },
   "outputs": [],
   "source": [
    "token_list = set()\n",
    "for i in X_tokens:\n",
    "    for j in i:\n",
    "        token_list.add(j)\n",
    "token_list = list(token_list)\n",
    "\n",
    "embeddings_subset = get_embeddings_subset(embeddings, token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "olJSAZbMjhxJ"
   },
   "outputs": [],
   "source": [
    "X_embeddings = get_embeddings_idx(X_tokens, embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FbrjdhpWjh2k"
   },
   "outputs": [],
   "source": [
    "X = np.array(X_embeddings, dtype=object)\n",
    "y = np.array(y_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing estimations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GFoxGGeomv5k"
   },
   "outputs": [],
   "source": [
    "# Function to shuffle and split/prepare the data\n",
    "def get_data(X, y):\n",
    "    # Shuffle the original data\n",
    "    shuffler = np.random.permutation(len(X))\n",
    "    X_shuffled = X[shuffler]\n",
    "    y_shuffled = y[shuffler]\n",
    "\n",
    "    trainxs = X_shuffled[:round(0.64 * X.shape[0])]\n",
    "    trainys = y_shuffled[:round(0.64 * X.shape[0])]\n",
    "    validxs = X_shuffled[round(0.64 * X.shape[0]):round(0.8 * X.shape[0])]\n",
    "    validys = y_shuffled[round(0.64 * X.shape[0]):round(0.8 * X.shape[0])]\n",
    "    testxs = X_shuffled[round(0.8 * X.shape[0]):]\n",
    "    testys = y_shuffled[round(0.8 * X.shape[0]):]\n",
    "\n",
    "    return trainxs, trainys, validxs, validys, testxs, testys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sm6no7vgjh8f"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, freeze_embeddings):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        weights = torch.FloatTensor(embeddings_subset.vectors)\n",
    "        self.embedding = nn.Embedding.from_pretrained(weights, freeze=freeze_embeddings)\n",
    "\n",
    "        self.fc1 = nn.Linear(300, 150)\n",
    "        self.fc2 = nn.Linear(150, 50)\n",
    "        self.fc3 = nn.Linear(50, 25)\n",
    "        self.fc4 = nn.Linear(25, 5)\n",
    "        self.fc5 = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        x = F.avg_pool1d(embedded, embedded.size(2)).squeeze(2)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTNvNjEVjh-Z"
   },
   "outputs": [],
   "source": [
    "# Function to get the model's loss on one specific dataset\n",
    "def get_scores(model, xs, ys):\n",
    "  criterion = nn.MSELoss()\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    train_loss_run = 0\n",
    "    for i in range(len(xs)):\n",
    "      x = torch.tensor(xs[i]).long().to(device)\n",
    "      y = torch.FloatTensor(ys[i]).to(device)\n",
    "      output = model(x).squeeze(1)\n",
    "      train_loss_run += torch.sqrt(criterion(output, y))\n",
    "    train_loss_run = train_loss_run / len(xs)\n",
    "  return train_loss_run.to('cpu').item()\n",
    "\n",
    "# Function to test the model with the defined metrics (RMSE-MAE-Spearman Corr)\n",
    "def test_model(model, xs, ys):\n",
    "  (xs_batches, ys_batches) = batchify_per_len(xs, ys, 128)\n",
    "  criterion_mse = nn.MSELoss()\n",
    "  criterion_mae = nn.L1Loss()\n",
    "\n",
    "  outputs = []\n",
    "  targets = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    loss_rmse = 0\n",
    "    loss_mae = 0\n",
    "    for i in range(len(xs_batches)):\n",
    "      x = torch.tensor(xs_batches[i]).long().to(device)\n",
    "      y = torch.FloatTensor(ys_batches[i]).to(device)\n",
    "      output = model(x).squeeze(1)\n",
    "      outputs.append(output)\n",
    "      targets.append(y)\n",
    "      loss_rmse += torch.sqrt(criterion_mse(output, y))\n",
    "      loss_mae += criterion_mae(output, y)\n",
    "    loss_rmse = loss_rmse / len(xs_batches)\n",
    "    loss_mae = loss_mae / len(xs_batches)\n",
    "  outputs = torch.cat(outputs)\n",
    "  targets = torch.cat(targets)\n",
    "  loss_corr = stats.spearmanr(targets.cpu().numpy(), outputs.cpu().numpy())[0]\n",
    "  print(\"Test Set --> RMSE Loss : {} / MAE Loss : {} / Spearman Correlation : {}\".format(loss_rmse, loss_mae, loss_corr))\n",
    "  return loss_rmse.to('cpu').item(), loss_mae.to('cpu').item(), loss_corr\n",
    "\n",
    "# Function to train a model\n",
    "def train_model(model, train_x, train_y, valid_x, valid_y, batch_size, learning_rate, max_epochs=1000, num_iterations=15, verbose=True):\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "  criterion = nn.MSELoss()\n",
    "\n",
    "  training_losses = []\n",
    "  validation_losses = []\n",
    "\n",
    "  min_loss = 10000\n",
    "  min_loss_epoch = None\n",
    "  best_model = None\n",
    "  convergence_counter = 0\n",
    "\n",
    "  epoch = 0\n",
    "  while (convergence_counter < num_iterations and epoch < max_epochs):\n",
    "    # Shuffle the training dataset\n",
    "    shuffler = list(np.random.permutation(len(train_x)))\n",
    "    train_x_shuffled = train_x[shuffler]\n",
    "    train_y_shuffled = train_y[shuffler]\n",
    "\n",
    "    # Batchify the data per length of sentence\n",
    "    (train_batches_x, train_batches_y) = batchify_per_len(train_x_shuffled, train_y_shuffled, batch_size)\n",
    "    (valid_batches_x, valid_batches_y) = batchify_per_len(valid_x, valid_y, batch_size)\n",
    "    \n",
    "    # Iterate through the batches and train\n",
    "    model.train()\n",
    "    for i in range(len(train_batches_x)):\n",
    "      x = torch.tensor(train_batches_x[i]).long().to(device)\n",
    "      y = torch.FloatTensor(train_batches_y[i]).to(device)\n",
    "      output = model(x).squeeze(1)\n",
    "      loss = criterion(output, y)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "    \n",
    "    # Evaluating the model\n",
    "    model.eval()\n",
    "    train_loss = get_scores(model, train_batches_x, train_batches_y)\n",
    "    valid_loss = get_scores(model, valid_batches_x, valid_batches_y)\n",
    "    # Storing and printing evaluation values\n",
    "    training_losses.append(train_loss)\n",
    "    validation_losses.append(valid_loss)\n",
    "    if (verbose):\n",
    "      print(\"Epoch {} | Training Loss : {} | Validation Loss : {}\".format(epoch, train_loss, valid_loss))\n",
    "    \n",
    "    # Checking current model's performance, and manage early stopping\n",
    "    if (valid_loss < min_loss):\n",
    "      min_loss = valid_loss\n",
    "      best_model = copy.deepcopy(model)\n",
    "      min_loss_epoch = epoch\n",
    "      convergence_counter = 0\n",
    "    else:\n",
    "      convergence_counter += 1\n",
    "    epoch += 1\n",
    "\n",
    "  # Plot the training/validation curves\n",
    "  plt.plot(training_losses, label=\"Training\")\n",
    "  plt.plot(validation_losses, label=\"Validation\")\n",
    "  plt.legend()\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Mean-Squared Error Loss')\n",
    "  plt.show()\n",
    "  print(\"Trained for {} epochs | Best Validation Loss : {} (Epoch : {})\".format(epoch, min_loss, min_loss_epoch))\n",
    "\n",
    "  return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xsVfHYSojiLj",
    "outputId": "d32848ef-3931-4548-ed9b-aa09137e1e31"
   },
   "outputs": [],
   "source": [
    "RMSEs, MAEs, Corrs = [], [], []\n",
    "for i in range(10):\n",
    "    trainxs, trainys, validxs, validys, testxs, testys = get_data(copy.deepcopy(X), copy.deepcopy(y))\n",
    "    # Change the freeze_embedding attribute to change between non-fine-tuned and fine-tuned\n",
    "    model = Net(freeze_embeddings=False).to(device)\n",
    "    trained_model = train_model(model, trainxs, trainys, validxs, validys, 128, 0.01, verbose=False)\n",
    "    rmse, mae, corr = test_model(trained_model, testxs, testys)\n",
    "    RMSEs.append(rmse)\n",
    "    MAEs.append(mae)\n",
    "    Corrs.append(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output the results to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "y_LEX5bz17Xu",
    "outputId": "5b7abfca-b547-4d97-9955-96002d2ce906"
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "results['RMSE'] = RMSEs\n",
    "results['MAE'] = MAEs\n",
    "results['Spearman Correlation'] = Corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fQGqf5WBjiQ2"
   },
   "outputs": [],
   "source": [
    "results.to_csv(\"../../data/temp_results/london_w2v_meanPooling_FineTune.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P5yJeKeXDw2W"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "London_Embeddings_MeanPooling_FineTuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
